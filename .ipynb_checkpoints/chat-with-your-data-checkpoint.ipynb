{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install langchain==0.1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Extractor from Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For filterout the warnings\n",
    "\n",
    "import warnings as wn\n",
    "wn.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 12:31:49.009700: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-28 12:31:49.035472: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-28 12:31:49.042533: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-28 12:31:50.531571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TextStreamer\n",
    "from transformers import Pipeline,pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junaid-ul-hassan/anaconda3/lib/python3.11/site-packages/langchain_huggingface/chat_models/__init__.py:1: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_huggingface.chat_models.huggingface import (\n"
     ]
    },
    {
     "ename": "SchemaError",
     "evalue": "Invalid Schema:\nmodel.config.extra_fields_behavior\n  Input should be 'allow', 'forbid' or 'ignore' [type=literal_error, input_value=<Extra.forbid: 'forbid'>, input_type=Extra]\n    For further information visit https://errors.pydantic.dev/2.9/v/literal_error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSchemaError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface_pipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFacePipeline\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEndpoint\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_huggingface/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatHuggingFace\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     HuggingFaceEmbeddings,\n\u001b[1;32m      4\u001b[0m     HuggingFaceEndpointEmbeddings,\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     HuggingFaceEndpoint,\n\u001b[1;32m      8\u001b[0m     HuggingFacePipeline,\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_huggingface/chat_models/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     TGI_MESSAGE,\n\u001b[1;32m      3\u001b[0m     TGI_RESPONSE,\n\u001b[1;32m      4\u001b[0m     ChatHuggingFace,\n\u001b[1;32m      5\u001b[0m     _convert_message_to_chat_message,\n\u001b[1;32m      6\u001b[0m     _convert_TGI_message_to_LC_message,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatHuggingFace\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_convert_message_to_chat_message\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTGI_RESPONSE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m ]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_huggingface/chat_models/huggingface.py:37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseTool\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction_calling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_to_openai_tool\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface_endpoint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEndpoint\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface_pipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFacePipeline\n\u001b[1;32m     40\u001b[0m DEFAULT_SYSTEM_PROMPT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are a helpful, respectful, and honest assistant.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_huggingface/llms/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface_endpoint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEndpoint\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface_pipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFacePipeline\n\u001b[1;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuggingFaceEndpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuggingFacePipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m ]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_huggingface/llms/huggingface_endpoint.py:25\u001b[0m\n\u001b[1;32m     15\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     17\u001b[0m VALID_TASKS \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext2text-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarization\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversational\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mHuggingFaceEndpoint\u001b[39;00m(LLM):\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    HuggingFace Endpoint.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     endpoint_url: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pydantic/_internal/_model_construction.py:224\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[0;34m(mcs, cls_name, bases, namespace, __pydantic_generic_metadata__, __pydantic_reset_parent_namespace__, _create_model_module, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_wrapper\u001b[38;5;241m.\u001b[39mfrozen \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__hash__\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m namespace:\n\u001b[1;32m    222\u001b[0m     set_default_hash_func(\u001b[38;5;28mcls\u001b[39m, bases)\n\u001b[0;32m--> 224\u001b[0m complete_model_class(\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m    226\u001b[0m     cls_name,\n\u001b[1;32m    227\u001b[0m     config_wrapper,\n\u001b[1;32m    228\u001b[0m     raise_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    229\u001b[0m     types_namespace\u001b[38;5;241m=\u001b[39mtypes_namespace,\n\u001b[1;32m    230\u001b[0m     create_model_module\u001b[38;5;241m=\u001b[39m_create_model_module,\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# If this is placed before the complete_model_class call above,\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# the generic computed fields return type is set to PydanticUndefined\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_computed_fields \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39minfo \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_decorators__\u001b[38;5;241m.\u001b[39mcomputed_fields\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pydantic/_internal/_model_construction.py:587\u001b[0m, in \u001b[0;36mcomplete_model_class\u001b[0;34m(cls, cls_name, config_wrapper, raise_errors, types_namespace, create_model_module)\u001b[0m\n\u001b[1;32m    584\u001b[0m core_config \u001b[38;5;241m=\u001b[39m config_wrapper\u001b[38;5;241m.\u001b[39mcore_config(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 587\u001b[0m     schema \u001b[38;5;241m=\u001b[39m gen_schema\u001b[38;5;241m.\u001b[39mclean_schema(schema)\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m gen_schema\u001b[38;5;241m.\u001b[39mCollectedInvalid:\n\u001b[1;32m    589\u001b[0m     set_model_mocks(\u001b[38;5;28mcls\u001b[39m, cls_name)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:595\u001b[0m, in \u001b[0;36mGenerateSchema.clean_schema\u001b[0;34m(self, schema)\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCollectedInvalid()\n\u001b[1;32m    594\u001b[0m schema \u001b[38;5;241m=\u001b[39m _discriminated_union\u001b[38;5;241m.\u001b[39mapply_discriminators(schema)\n\u001b[0;32m--> 595\u001b[0m schema \u001b[38;5;241m=\u001b[39m validate_core_schema(schema)\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m schema\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pydantic/_internal/_core_utils.py:570\u001b[0m, in \u001b[0;36mvalidate_core_schema\u001b[0;34m(schema)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPYDANTIC_SKIP_VALIDATING_CORE_SCHEMAS\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m schema\n\u001b[0;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _validate_core_schema(schema)\n",
      "\u001b[0;31mSchemaError\u001b[0m: Invalid Schema:\nmodel.config.extra_fields_behavior\n  Input should be 'allow', 'forbid' or 'ignore' [type=literal_error, input_value=<Extra.forbid: 'forbid'>, input_type=Extra]\n    For further information visit https://errors.pydantic.dev/2.9/v/literal_error"
     ]
    }
   ],
   "source": [
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain_huggingface import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'api_token'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapi_token\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Falcon_API\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'api_token'"
     ]
    }
   ],
   "source": [
    "from api_token import Falcon_API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = Falcon_API()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = api.get_Key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junaid-ul-hassan/anaconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/junaid-ul-hassan/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    name=\"WEB_PILOT\",\n",
    "    huggingfacehub_api_token=api_key,\n",
    "    repo_id= 'mistralai/Mistral-7B-Instruct-v0.3',\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=500,\n",
    "    temperature=0.9\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages from langchains for llm\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap Data from Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# import python packages to scrap data from sites\n",
    "\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap text from Websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidSchema",
     "evalue": "No connection adapters were found for 'chrome://newtab/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidSchema\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccept\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m }\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Send a GET request to the URL with headers\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39mheaders)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:697\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    694\u001b[0m hooks \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mhooks\n\u001b[1;32m    696\u001b[0m \u001b[38;5;66;03m# Get the appropriate adapter to use\u001b[39;00m\n\u001b[0;32m--> 697\u001b[0m adapter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_adapter(url\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    699\u001b[0m \u001b[38;5;66;03m# Start time (approximately) of the request\u001b[39;00m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:794\u001b[0m, in \u001b[0;36mSession.get_adapter\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m adapter\n\u001b[1;32m    793\u001b[0m \u001b[38;5;66;03m# Nothing matches :-/\u001b[39;00m\n\u001b[0;32m--> 794\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidSchema(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo connection adapters were found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mInvalidSchema\u001b[0m: No connection adapters were found for 'chrome://newtab/'"
     ]
    }
   ],
   "source": [
    "url = \"chrome://newtab/\"\n",
    "\n",
    "# Define the headers to mimic a browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'\n",
    "}\n",
    "\n",
    "# Send a GET request to the URL with headers\n",
    "response = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 Heading: No h1 heading found\n"
     ]
    }
   ],
   "source": [
    "if response.status_code == 200:\n",
    "    # Parse the HTML content using Beautiful Soup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extracting the h1 heading of the webpage\n",
    "    h1_heading = soup.find('h1').text.strip() if soup.find('h1') else \"No h1 heading found\"\n",
    "    print(\"h1 Heading:\", h1_heading)\n",
    "    \n",
    "    # Extracting all the text content\n",
    "    all_text = soup.get_text(separator=\"\\n\").strip()\n",
    "    \n",
    "    # Extracting the table text\n",
    "    table_rows = soup.find_all(['tr','li','strong','span'])\n",
    "    table_text = \"\\n\".join([row.get_text(separator=\" \").strip() for row in table_rows])\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_string = all_text+table_text\n",
    "lines = [line for line in original_string.splitlines() if line.strip()]\n",
    "# Join the lines back together without empty lines\n",
    "result_string = ' '.join(lines)\n",
    "\n",
    "# Print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_text = re.sub(r'\\s+', ' ', result_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NUML UNIVERSITY | Program | NUML Journals Web Mail NEWS CMS Admissions Home Dapartment About Programs BS Computer Science (BSCS) MS Computer Science (MS-CS) PhD Computer Science (PhD-CS) BS Artificial Intelligence (BS AI) MS (Artificial Intelligence) MS (Cyber Security) Associate Degree in Computing Students Related Final Project SOP Project Tracks Completed Projects Winning Projects FYP Calendar Student Community Timetable Datesheet Result Contact Toggle navigation Faculty Site | Home About About Us Objectives Values Programs BS Computer Science (BSCS) MS Computer Science (MS-CS) PhD Computer Science (PhD-CS) BS Artificial Intelligence (BS AI) MS (Artificial Intelligence) MS (Cyber Security) Associate Degree in Computing Research Research Summary Research Groups Artificial Intelligence Research Group Data Science & Analytics Research Group Emerging Trends in Computer Science Research Group Cyber Security and Network Applications Research Group Research alliances Research opportunities'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_text[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_text_to_file(text, filename):\n",
    "    words = text.split()\n",
    "    formatted_text = re.sub(r'\\s+', ' ', result_string)\n",
    "    with open(filename, 'w') as file:\n",
    "        for i in range(0, len(words), 20):\n",
    "            file.write(' '.join(words[i:i+20]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_text_to_file(formatted_text,filename='web_content.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now load the document\n",
    "\n",
    "loader = TextLoader(\n",
    "    file_path='web_content.txt'\n",
    ")\n",
    "\n",
    "# loader = WebBaseLoader(\n",
    "#     web_path = web_path,\n",
    "#     header_template = None,\n",
    "#     verify_ssl = True,\n",
    "#     proxies = None,\n",
    "#     continue_on_failure = False,\n",
    "#     requests_per_second = 2,\n",
    "#     default_parser = 'html.parser'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()\n",
    "txt_data = docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23437"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": Credit Hrs: 138\n",
      "Credit Hrs: Semesters: 8 Semesters: BS Computer Science (BSCS) MS Computer Science (MS-CS) PhD Computer Science (PhD-CS) BS Artificial Intelligence\n",
      "(BS AI) MS (Artificial Intelligence) MS (Cyber Security) Associate Degree in Computing National University of Modern Languages H-9 Islamabad ,Pakistan\n",
      "+92-51-9265100 info@numl.edu.pk E-library Results Student Clearance NUML RTI Compliance Placement System Success Stories LMS CMS Scholarships Publications System HR System\n",
      "Datesheets\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(txt_data[-500:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now define the chunks and overlap\n",
    "\n",
    "chunks_size = 1200\n",
    "chunks_overlap = 200\n",
    "\n",
    "rec_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"],\n",
    "    chunk_size=chunks_size,\n",
    "    chunk_overlap=chunks_overlap,\n",
    "    is_separator_regex=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NUML UNIVERSITY | Program | NUML Journals Web Mail NEWS CMS Admissions Home Dapartment About Programs BS Computer Science (BSCS)\\nMS Computer Science (MS-CS) PhD Computer Science (PhD-CS) BS Artificial Intelligence (BS AI) MS (Artificial Intelligence) MS (Cyber Security) Associate\\nDegree in Computing Students Related Final Project SOP Project Tracks Completed Projects Winning Projects FYP Calendar Student Community Timetable Datesheet\\nResult Contact Toggle navigation Faculty Site | Home About About Us Objectives Values Programs BS Computer Science (BSCS) MS Computer\\nScience (MS-CS) PhD Computer Science (PhD-CS) BS Artificial Intelligence (BS AI) MS (Artificial Intelligence) MS (Cyber Security) Associate Degree in\\nComputing Research Research Summary Research Groups Artificial Intelligence Research Group Data Science & Analytics Research Group Emerging Trends in Computer\\nScience Research Group Cyber Security and Network Applications Research Group Research alliances Research opportunities']"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_splitter.split_text(txt_data[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now try the Simple charater splitter\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator='\\n',\n",
    "    chunk_size=chunks_size,\n",
    "    chunk_overlap = chunks_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NUML UNIVERSITY | Program | NUML Journals Web Mail NEWS CMS Admissions Home Dapartment About Programs BS Computer Science (BSCS)\\nMS Computer Science (MS-CS) PhD Computer Science (PhD-CS) BS Artificial Intelligence (BS AI) MS (Artificial Intelligence) MS (Cyber Security) Associate\\nDegree in Computing Students Related Final Project SOP Project Tracks Completed Projects Winning Projects FYP Calendar Student Community Timetable Datesheet\\nResult Contact Toggle navigation Faculty Site | Home About About Us Objectives Values Programs BS Computer Science (BSCS) MS Computer\\nScience (MS-CS) PhD Computer Science (PhD-CS) BS Artificial Intelligence (BS AI) MS (Artificial Intelligence) MS (Cyber Security) Associate Degree in\\nComputing Research Research Summary Research Groups Artificial Intelligence Research Group Data Science & Analytics Research Group Emerging Trends in Computer\\nScience Research Group Cyber Security and Network Applications Research Group Research alliances Research opportunities Research Excellence Award Department Research Scholars',\n",
       " 'Science Research Group Cyber Security and Network Applications Research Group Research alliances Research opportunities Research Excellence Award Department Research Scholars\\nResearch Students Related Final Project SOP Project Tracks FYP Archive Winning Projects FYP Calendar Student Community Club & Societies Downloads\\nTimetable Datasheet Results Faculty Members Contact Home Department About Programs BS Computer Science (BSCS) MS Computer Science (MS-CS) PhD Computer\\nScience (PhD-CS) BS Artificial Intelligence (BS AI) MS (Artificial Intelligence) MS (Cyber Security) Associate Degree in Computing Research Researc']"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter.split_text(txt_data[0: 1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = rec_splitter.split_documents(\n",
    "    docs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name='all-MiniLM-L6-v2'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1= \"I love dogs\"\n",
    "t2 = \"I love cats\"\n",
    "t3 = \"Study is very hard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "embd1 = embedding.embed_query(t1)\n",
    "embd2 = embedding.embed_query(t2)\n",
    "embd3 = embedding.embed_query(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7720985168186761, 0.06933482198571146)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embd1,embd2), np.dot(embd2,embd3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Vector Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.vectorstores import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(\n",
    "    documents= split,\n",
    "    embedding=embedding,\n",
    "    collection_name='langchain',\n",
    "    persist_directory='docs/chroma/',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Merit List'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='in Computing Associate Degree in Computing Department Of Software Engineering BS Software Engineering BS Software Engineering Department Of Applied Psychology\\nBS Psychology BS Psychology Department Of Languages Pakistan Studies Islamic Studies Mathematics Physics Admission Gallery Scholarships Jobs Downloads Tenders B\\nICON Traning Feedback Form Examination Rules – 2023 Alumni Employment Survey 2024 Psychological Counselling Centre Directory of Theses Degree Attestation\\nSystem NUML Faculty Abstract of Theses Degree Equivalence System BICON - Idea Registration B ICON Trainers/mentors Form Access of Publications\\nThesis Defence E - Registration Datesheets Hostels Form Transportal Success Stories NEWS CMS High Achievers NUML RTI Compliance Placement System\\nE-library Quetta Quetta Mirpur AJK Mirpur AJK Faisalabad Faisalabad Karachi Karachi Peshawar Peshawar Hyderabad Hyderabad Multan Multan Lahore Lahore Rawalpindi\\nRawalpindi 2024 © NUML All Rights Reserved developed by ICT Department | Privacy Policy', metadata={'source': 'web_content.txt'}),\n",
       " Document(page_content='Examinations Rules - 2023 Disability Coordinators Life @ NUML Social Media Facebook Twitter Youtube Linkedin Instagram Gallery Facilities Societies Success\\nStories Climate Action Plan Repository Annual Reports Academic Calendars Event Repository SDGs Report Home BS Programs BS Programs Main Department\\nOf Educational Sciences B.Ed (Hons.) 4 Year B.Ed (Bridging) BS Early childhood Care and Education B.Ed 1.5 year BS Health\\nand Physical Education BS Educational Leadership and Management Department Of International Relations BS International Relations Department Of Peace & Conflict\\nStudies BS Peace & Conflict Studies BS Peace & Conflict Studies (Bridging) Department Of Media And Communication Studies BS Media\\n& Communication Studies Department Of Pakistan Studies BS - History BS Pakistan Studies BS Pakistan Studies (Bridging) BS History Department\\nOf Islamic Thought And Culture BS Islamic Studies BS Islamic Studies (Bridging) Department Of Computer Science BS Computer Science (BSCS)\\nBS Artificial Intelligence (BS AI) Associate Degree in Computing Department Of Electrical Engineering BE Electrical Engineering BS Electronics Department Of', metadata={'source': 'web_content.txt'}),\n",
       " Document(page_content=\"Property Rights (IPR) Policy (2020) Examinations Rules - 2023 SOP - NUML Official Accounts / Domains View All Policies Research\\nJournals Projects Thesis Defence Thesis Repository Publications Newsletters Books Annual Reports Libraries OPAC Digital Library (HEC) Off-Campus Access to Digital\\nLibrary NUML Live NUML FM Radio NUML TV Tenders Tenders Jobs Jobs Contact Us Contact Us Admissions Jean Monnet Chair\\nRector's Suggestion Box Student Portal Translation Services Latest Events Home Home About History Core Values Vision and Mission Objectives Board\\nof Governors Rector Director General About History History History History Core Values Core Values Core Values Core Values Vision and\\nMission Vision and Mission Vision and Mission Vision and Mission Objectives Objectives Objectives Objectives Board of Governors Board of Governors\\nBoard of Governors Board of Governors Rector Rector Rector Rector Director General Director General Director General Director General Campuses Faisalabad\\nHyderabad Karachi Lahore Mirpur (Azad Kashmir) Multan Peshawar Quetta Rawalpindi Campuses Faisalabad Faisalabad Faisalabad Faisalabad Hyderabad Hyderabad Hyderabad Hyderabad Karachi\", metadata={'source': 'web_content.txt'})]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search(\n",
    "    query=query,\n",
    "    k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidCollectionException",
     "evalue": "Collection 478491d2-e769-460f-8af1-682937b01a08 does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidCollectionException\u001b[0m                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[200], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# using MMR\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m mmr_docs \u001b[38;5;241m=\u001b[39m db\u001b[38;5;241m.\u001b[39mmax_marginal_relevance_search(\n\u001b[1;32m      4\u001b[0m     query\u001b[38;5;241m=\u001b[39mquery,\n\u001b[1;32m      5\u001b[0m     k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m      6\u001b[0m     fetch_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:562\u001b[0m, in \u001b[0;36mChroma.max_marginal_relevance_search\u001b[0;34m(self, query, k, fetch_k, lambda_mult, filter, where_document, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor MMR search, you must specify an embedding function on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreation.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m     )\n\u001b[1;32m    561\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function\u001b[38;5;241m.\u001b[39membed_query(query)\n\u001b[0;32m--> 562\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_marginal_relevance_search_by_vector(\n\u001b[1;32m    563\u001b[0m     embedding,\n\u001b[1;32m    564\u001b[0m     k,\n\u001b[1;32m    565\u001b[0m     fetch_k,\n\u001b[1;32m    566\u001b[0m     lambda_mult\u001b[38;5;241m=\u001b[39mlambda_mult,\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfilter\u001b[39m,\n\u001b[1;32m    568\u001b[0m     where_document\u001b[38;5;241m=\u001b[39mwhere_document,\n\u001b[1;32m    569\u001b[0m )\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:509\u001b[0m, in \u001b[0;36mChroma.max_marginal_relevance_search_by_vector\u001b[0;34m(self, embedding, k, fetch_k, lambda_mult, filter, where_document, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax_marginal_relevance_search_by_vector\u001b[39m(\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    483\u001b[0m     embedding: List[\u001b[38;5;28mfloat\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    490\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m    491\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return docs selected using the maximal marginal relevance.\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;124;03m    Maximal marginal relevance optimizes for similarity to query AND diversity\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;124;03m    among selected documents.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;124;03m        List of Documents selected by maximal marginal relevance.\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__query_collection(\n\u001b[1;32m    510\u001b[0m         query_embeddings\u001b[38;5;241m=\u001b[39membedding,\n\u001b[1;32m    511\u001b[0m         n_results\u001b[38;5;241m=\u001b[39mfetch_k,\n\u001b[1;32m    512\u001b[0m         where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfilter\u001b[39m,\n\u001b[1;32m    513\u001b[0m         where_document\u001b[38;5;241m=\u001b[39mwhere_document,\n\u001b[1;32m    514\u001b[0m         include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadatas\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistances\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    515\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    516\u001b[0m     )\n\u001b[1;32m    517\u001b[0m     mmr_selected \u001b[38;5;241m=\u001b[39m maximal_marginal_relevance(\n\u001b[1;32m    518\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(embedding, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m    519\u001b[0m         results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    520\u001b[0m         k\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m    521\u001b[0m         lambda_mult\u001b[38;5;241m=\u001b[39mlambda_mult,\n\u001b[1;32m    522\u001b[0m     )\n\u001b[1;32m    524\u001b[0m     candidates \u001b[38;5;241m=\u001b[39m _results_to_docs(results)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/utils/utils.py:36\u001b[0m, in \u001b[0;36mxor_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m     invalid_group_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(arg_groups[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m invalid_groups]\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExactly one argument in each of the following\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m groups must be defined:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(invalid_group_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m     )\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py:156\u001b[0m, in \u001b[0;36mChroma.__query_collection\u001b[0;34m(self, query_texts, query_embeddings, n_results, where, where_document, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import chromadb python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install chromadb`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m     )\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection\u001b[38;5;241m.\u001b[39mquery(\n\u001b[1;32m    157\u001b[0m     query_texts\u001b[38;5;241m=\u001b[39mquery_texts,\n\u001b[1;32m    158\u001b[0m     query_embeddings\u001b[38;5;241m=\u001b[39mquery_embeddings,\n\u001b[1;32m    159\u001b[0m     n_results\u001b[38;5;241m=\u001b[39mn_results,\n\u001b[1;32m    160\u001b[0m     where\u001b[38;5;241m=\u001b[39mwhere,\n\u001b[1;32m    161\u001b[0m     where_document\u001b[38;5;241m=\u001b[39mwhere_document,\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    163\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/chromadb/api/models/Collection.py:195\u001b[0m, in \u001b[0;36mCollection.query\u001b[0;34m(self, query_embeddings, query_texts, query_images, query_uris, n_results, where, where_document, include)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the n_results nearest neighbor embeddings for provided query_embeddings or query_texts.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    176\u001b[0m \n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    179\u001b[0m (\n\u001b[1;32m    180\u001b[0m     valid_query_embeddings,\n\u001b[1;32m    181\u001b[0m     valid_n_results,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m     include,\n\u001b[1;32m    193\u001b[0m )\n\u001b[0;32m--> 195\u001b[0m query_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_query(\n\u001b[1;32m    196\u001b[0m     collection_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m    197\u001b[0m     query_embeddings\u001b[38;5;241m=\u001b[39mvalid_query_embeddings,\n\u001b[1;32m    198\u001b[0m     n_results\u001b[38;5;241m=\u001b[39mvalid_n_results,\n\u001b[1;32m    199\u001b[0m     where\u001b[38;5;241m=\u001b[39mvalid_where,\n\u001b[1;32m    200\u001b[0m     where_document\u001b[38;5;241m=\u001b[39mvalid_where_document,\n\u001b[1;32m    201\u001b[0m     include\u001b[38;5;241m=\u001b[39minclude,\n\u001b[1;32m    202\u001b[0m )\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_query_response(query_results, include)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/chromadb/telemetry/opentelemetry/__init__.py:146\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/chromadb/rate_limiting/__init__.py:47\u001b[0m, in \u001b[0;36mrate_limit.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Dict[Any, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# If not rate limiting provider is present, just run and return the function.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_system\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mchroma_rate_limiting_provider_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m subject \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m     50\u001b[0m         subject_value \u001b[38;5;241m=\u001b[39m kwargs[subject]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/chromadb/api/segment.py:695\u001b[0m, in \u001b[0;36mSegmentAPI._query\u001b[0;34m(self, collection_id, query_embeddings, n_results, where, where_document, include)\u001b[0m\n\u001b[1;32m    687\u001b[0m where_document \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    688\u001b[0m     validate_where_document(where_document)\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m where_document \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(where_document) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m where_document\n\u001b[1;32m    691\u001b[0m )\n\u001b[1;32m    693\u001b[0m allowed_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 695\u001b[0m coll \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_collection(collection_id)\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m query_embeddings:\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_dimension(coll, \u001b[38;5;28mlen\u001b[39m(embedding), update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/chromadb/telemetry/opentelemetry/__init__.py:146\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/chromadb/api/segment.py:852\u001b[0m, in \u001b[0;36mSegmentAPI._get_collection\u001b[0;34m(self, collection_id)\u001b[0m\n\u001b[1;32m    850\u001b[0m     collections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sysdb\u001b[38;5;241m.\u001b[39mget_collections(\u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mcollection_id)\n\u001b[1;32m    851\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m collections:\n\u001b[0;32m--> 852\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidCollectionException(\n\u001b[1;32m    853\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollection \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    854\u001b[0m         )\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection_cache[collection_id] \u001b[38;5;241m=\u001b[39m collections[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection_cache[collection_id]\n",
      "\u001b[0;31mInvalidCollectionException\u001b[0m: Collection 478491d2-e769-460f-8af1-682937b01a08 does not exist."
     ]
    }
   ],
   "source": [
    "# using MMR\n",
    "\n",
    "mmr_docs = db.max_marginal_relevance_search(\n",
    "    query=query,\n",
    "    k=2,\n",
    "    fetch_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmr_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compressor Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T20:07:17.169795Z",
     "iopub.status.busy": "2024-06-28T20:07:17.168924Z",
     "iopub.status.idle": "2024-06-28T20:07:17.173810Z",
     "shell.execute_reply": "2024-06-28T20:07:17.172846Z",
     "shell.execute_reply.started": "2024-06-28T20:07:17.169764Z"
    }
   },
   "outputs": [],
   "source": [
    "# meta_data_field_info = [\n",
    "#     AttributeInfo(\n",
    "#         name='source',\n",
    "#         description='Text chunks should be one of `/docs/use_cases/query_analysis/`',\n",
    "#         type='string'\n",
    "#     ),\n",
    "#     AttributeInfo(\n",
    "#         name='source',\n",
    "#         description='The heading from contexts',\n",
    "#         type='string'\n",
    "#     )\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T20:07:17.330624Z",
     "iopub.status.busy": "2024-06-28T20:07:17.330257Z",
     "iopub.status.idle": "2024-06-28T20:07:17.334880Z",
     "shell.execute_reply": "2024-06-28T20:07:17.333895Z",
     "shell.execute_reply.started": "2024-06-28T20:07:17.330597Z"
    }
   },
   "outputs": [],
   "source": [
    "# docs_content_disc = 'Langchain RAG'\n",
    "\n",
    "# ret = SelfQueryRetriever.from_llm(\n",
    "#     llm,\n",
    "#     db,\n",
    "#     docs_content_disc,\n",
    "#     meta_data_field_info,\n",
    "#     verbose=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T20:07:25.030022Z",
     "iopub.status.busy": "2024-06-28T20:07:25.029657Z",
     "iopub.status.idle": "2024-06-28T20:07:25.035364Z",
     "shell.execute_reply": "2024-06-28T20:07:25.034363Z",
     "shell.execute_reply.started": "2024-06-28T20:07:25.029994Z"
    }
   },
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T20:07:34.916294Z",
     "iopub.status.busy": "2024-06-28T20:07:34.915927Z",
     "iopub.status.idle": "2024-06-28T20:07:34.921278Z",
     "shell.execute_reply": "2024-06-28T20:07:34.920363Z",
     "shell.execute_reply.started": "2024-06-28T20:07:34.916264Z"
    }
   },
   "outputs": [],
   "source": [
    "compressor = LLMChainExtractor.from_llm(\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T20:07:39.119977Z",
     "iopub.status.busy": "2024-06-28T20:07:39.119615Z",
     "iopub.status.idle": "2024-06-28T20:07:39.125135Z",
     "shell.execute_reply": "2024-06-28T20:07:39.123878Z",
     "shell.execute_reply.started": "2024-06-28T20:07:39.119950Z"
    }
   },
   "outputs": [],
   "source": [
    "compressor_ret = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=db.as_retriever(\n",
    "        search_type = 'mmr'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T20:08:00.734086Z",
     "iopub.status.busy": "2024-06-28T20:08:00.733692Z",
     "iopub.status.idle": "2024-06-28T20:08:00.738444Z",
     "shell.execute_reply": "2024-06-28T20:08:00.737384Z",
     "shell.execute_reply.started": "2024-06-28T20:08:00.734055Z"
    }
   },
   "outputs": [],
   "source": [
    "question = \"Why we need Gemma?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T19:15:19.507466Z",
     "iopub.status.busy": "2024-05-06T19:15:19.507064Z",
     "iopub.status.idle": "2024-05-06T19:15:19.511698Z",
     "shell.execute_reply": "2024-05-06T19:15:19.510864Z",
     "shell.execute_reply.started": "2024-05-06T19:15:19.507435Z"
    }
   },
   "outputs": [],
   "source": [
    "# compressed_docs = compressor_ret.get_relevant_documents(\n",
    "#     query=question\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T19:15:22.612604Z",
     "iopub.status.busy": "2024-05-06T19:15:22.611941Z",
     "iopub.status.idle": "2024-05-06T19:15:22.656168Z",
     "shell.execute_reply": "2024-05-06T19:15:22.654980Z",
     "shell.execute_reply.started": "2024-05-06T19:15:22.612570Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compressed_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pretty_print_docs(\u001b[43mcompressed_docs\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compressed_docs' is not defined"
     ]
    }
   ],
   "source": [
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM + RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this function is for clear chromadb\n",
    "\n",
    "# import shutil\n",
    "\n",
    "# def delete_all(directory):\n",
    "#     try:\n",
    "#         shutil.rmtree(directory)\n",
    "#         print(f\"All files and subdirectories in {directory} have been deleted.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to delete {directory}: {e}\")\n",
    "\n",
    "# # Usage\n",
    "# directory_path = '/kaggle/working/docs/'\n",
    "# delete_all(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt template for LLM\n",
    "\n",
    "template = \"\"\"You are a chatbot designed to answer user questions based on the provided context. \n",
    "If you don't know the answer, say \"I don't know\" and don't make up an answer. \n",
    "Use a maximum of ten sentences and keep your answer informative. \n",
    "Always end with \"Thanks for asking!\"\n",
    "\n",
    "{chat_history}\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain_prompt = PromptTemplate.from_template(\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = ConversationBufferMemory(\n",
    "     memory_key='chat_history', \n",
    "    return_messages=False, \n",
    "    human_prefix='Human',\n",
    "    ai_prefix='AI',\n",
    "    input_key='question',\n",
    "    verbose=False,\n",
    ")\n",
    "window_mem = ConversationBufferWindowMemory(\n",
    "    k=5,\n",
    "    memory_key='chat_history', \n",
    "    return_messages=False, \n",
    "#   output_key='answer',\n",
    "    human_prefix='Human',\n",
    "    ai_prefix='AI',\n",
    "    input_key = 'question',\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever = db.as_retriever(\n",
    "        search_type='mmr',\n",
    "        search_kwargs={'k': 4, 'fetch_k': 50}\n",
    "    ),\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": qa_chain_prompt,\n",
    "        \"verbose\": True,\n",
    "        \"memory\": window_mem\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How many campuses of NUML?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:chromadb.segment.impl.vector.local_persistent_hnsw:Number of requested results 50 is greater than number of elements in index 38, updating n_results = 38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a chatbot designed to answer user questions based on the provided context. \n",
      "If you don't know the answer, say \"I don't know\" and don't make up an answer. \n",
      "Use a maximum of ten sentences and keep your answer informative. \n",
      "Always end with \"Thanks for asking!\"\n",
      "\n",
      "Human: Tell names of Undergradute programs NUML Offers in main Campus. tell me in bullets\n",
      "AI: 1. BS Computer Science\n",
      "2. BS Artificial Intelligence\n",
      "3. BS Software Engineering\n",
      "4. BS Information Technology\n",
      "5. BS Media & Communication Studies\n",
      "6. BS Accounting and Finance\n",
      "7. BS English (Bridging)\n",
      "8. BS Psychology\n",
      "9. B.Ed (Hons.) 4 Year (Main Campus)\n",
      "10. B.Ed (Bridging) (Main Campus)\n",
      "\n",
      "Thanks for asking!\n",
      "\n",
      "Context: Programs | NUML NATIONAL UNIVERSITY OF MODERN LANGUAGES LMS Policies NUML Policy on Protection Against Sexual Harassment (2020) HEC Policy\n",
      "on Protection Against Sexual Harassment (2020) Handout – Harassment Policy 2020 Assessment/Examination and Admission Policy COVID-19 NUML Online Education Policy\n",
      "Policy Guidelines for International Linkages (2017) NUML Right of Access to Information Policy (2018) Conflict of Interest Policy (2023) Intellectual\n",
      "Property Rights (IPR) Policy (2020) Examinations Rules - 2023 SOP - NUML Official Accounts / Domains View All Policies Research\n",
      "Journals Projects Thesis Defence Thesis Repository Publications Newsletters Books Annual Reports Libraries OPAC Digital Library (HEC) Off-Campus Access to Digital\n",
      "Library NUML Live NUML FM Radio NUML TV Tenders Jobs Contact Us NUML Admissions Jean Monnet Chair Rector's Suggestion Box\n",
      "Student Portal Translation Services Latest Events Home About History Core Values Vision and Mission Objectives Board of Governors Rector Director\n",
      "General Campuses Faisalabad Hyderabad Karachi Lahore Mirpur (Azad Kashmir) Multan Peshawar Quetta Rawalpindi Faculties Faculty of Engineering and Computing Department\n",
      "\n",
      "Sejong Institute Admissions Admissions Home Admissions Admissions Home Admissions Home Admissions Home Admissions Home Online Admissions Online Admissions Online Admissions\n",
      "Online Admissions Fee Structure Fee Structure Fee Structure Fee Structure Eligibility Criteria Eligibility Criteria Eligibility Criteria Eligibility Criteria Admission Schedule\n",
      "Admission Schedule Admission Schedule Admission Schedule Academic Calendar Academic Calendar Academic Calendar Academic Calendar Advertisement Advertisement Advertisement Advertisement Model Papers\n",
      "Model Papers Model Papers Model Papers FAQs FAQs FAQs FAQs Academics Academics Branch Student Services Admission Policy / Procedure Migration\n",
      "/ Credit Transfer Fee Refund Change of Shift Freezing of Semester Students Mandatory Attendance Transfer of Students within NUML Campuses\n",
      "Reserved Seats & Scholarship Policy Repeat / NOC/Bonafied Certificate Procedure Undergraduate Programs Post Graduate Programs PhD Programs Functional Courses Diplomas\n",
      "Online Teaching Downloads Proforma for Migration / NOC Proforma for Attendance / Character Certificate Proforma for Campus Change Proforma for\n",
      "\n",
      "Hyderabad Karachi Lahore Mirpur (Azad Kashmir) Multan Peshawar Quetta Rawalpindi Campuses Faisalabad Faisalabad Faisalabad Faisalabad Hyderabad Hyderabad Hyderabad Hyderabad Karachi\n",
      "Karachi Karachi Karachi Lahore Lahore Lahore Lahore Mirpur (Azad Kashmir) Mirpur (Azad Kashmir) Mirpur (Azad Kashmir) Mirpur (Azad Kashmir) Multan\n",
      "Multan Multan Multan Peshawar Peshawar Peshawar Peshawar Quetta Quetta Quetta Quetta Rawalpindi Rawalpindi Rawalpindi Rawalpindi Faculties Faculty of Engineering and\n",
      "Computing Department of Computer Science Department of Electrical Engineering Department of Mathematics Department of Software Engineering Faculty of Languages Department\n",
      "of Arabic Department of Chinese Department of English Language Teaching (ELT) Department of French Department of German Department of Italian\n",
      "Department of Japanese Department of Korean Language & Culture Department of Pakistani Languages Department of Persian Department of Russian Department\n",
      "of South Asian Languages Department of Spanish Department of Translation & Interpretation Department of Turkish Studies Department of Urdu Faculty\n",
      "\n",
      "Coordinators Disability Coordinators Life @ NUML Social Media Facebook Twitter Youtube Linkedin Instagram Gallery Facilities Societies Success Stories Climate Action\n",
      "Plan Life @ NUML Social Media Facebook Twitter Youtube Linkedin Instagram Social Media Facebook Facebook Twitter Twitter Youtube Youtube Linkedin\n",
      "Linkedin Instagram Instagram Gallery Gallery Gallery Gallery Facilities Facilities Facilities Facilities Societies Societies Societies Societies Success Stories Success Stories Success\n",
      "Stories Success Stories Climate Action Plan Climate Action Plan Climate Action Plan Climate Action Plan Repository Annual Reports Academic Calendars\n",
      "Event Repository SDGs Report Repository Annual Reports Annual Reports Academic Calendars Academic Calendars Event Repository Event Repository SDGs Report SDGs\n",
      "Report Home BS Programs Department Of Educational Sciences B.Ed (Hons.) 4 Year B.Ed (Hons.) 4 Year B.Ed (Bridging) B.Ed (Bridging)\n",
      "BS Early childhood Care and Education BS Early childhood Care and Education B.Ed 1.5 year B.Ed 1.5 year BS Health\n",
      "Question: How many campuses of NUML?\n",
      "Answer:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "1. Faisalabad\n",
      "2. Hyderabad\n",
      "3. Karachi\n",
      "4. Lahore\n",
      "5. Mirpur (Azad Kashmir)\n",
      "6. Multan\n",
      "7. Peshawar\n",
      "8. Quetta\n",
      "9. Rawalpindi\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain.invoke({'query':question})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T20:16:15.106805Z",
     "iopub.status.busy": "2024-06-28T20:16:15.106408Z",
     "iopub.status.idle": "2024-06-28T20:16:59.322694Z",
     "shell.execute_reply": "2024-06-28T20:16:59.321414Z",
     "shell.execute_reply.started": "2024-06-28T20:16:15.106766Z"
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask:  Gemma 2 vs Llama 3 which is better?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Gemma 2 is better than Llama 3 in terms of performance.\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAsk: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     result \u001b[38;5;241m=\u001b[39m qa_chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m:question})\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    question = input(\"Ask: \")\n",
    "    result = qa_chain.invoke({'query':question})\n",
    "    print(result[\"result\"])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Tell names of Undergradute programs NUML Offers in main Campus. tell me in bullets\n",
      "AI: 1. BS Computer Science\n",
      "2. BS Artificial Intelligence\n",
      "3. BS Software Engineering\n",
      "4. BS Information Technology\n",
      "5. BS Media & Communication Studies\n",
      "6. BS Accounting and Finance\n",
      "7. BS English (Bridging)\n",
      "8. BS Psychology\n",
      "9. B.Ed (Hons.) 4 Year (Main Campus)\n",
      "10. B.Ed (Bridging) (Main Campus)\n",
      "\n",
      "Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "print(window_mem.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_mem.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self):\n",
    "        self.previous_url = None\n",
    "        \n",
    "    \n",
    "    def __write_txt_file(self, text, string):\n",
    "        # Define the file path\n",
    "        file_path = \"/media/junaid-ul-hassan/248ac48e-ccd4-4707-a28b-33cb7a46e6dc/LLMs Projects/Web_pilot/text_file.txt/text_file.txt\"\n",
    "        line_length = 20\n",
    "        \n",
    "        # Format the text\n",
    "        words = text.split()\n",
    "        formatted_text = re.sub(r'\\s+', ' ', string)\n",
    "\n",
    "        # Open the file in write mode to clear its content and write new data\n",
    "        with open(file_path, 'w') as file:\n",
    "            for i in range(0, len(words), line_length):\n",
    "                file.write(' '.join(words[i:i + line_length]) + '\\n')\n",
    "    \n",
    "    def is_new_url(self, new_url):\n",
    "        if self.previous_url is not None and self.previous_url == new_url:\n",
    "            return False\n",
    "        else:\n",
    "            self.previous_url = new_url\n",
    "            return True\n",
    "    \n",
    "    def scrape_website(self,url):\n",
    "        # Define the headers to mimic a browser request\n",
    "        \n",
    "        if self.is_new_url(\n",
    "            new_url=url\n",
    "        ):\n",
    "            # self.rag.window_mem.clear()\n",
    "            pass\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # headers = {\n",
    "        #     'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        #     'Accept-Encoding':'gzip, deflate, br',\n",
    "        #     'Accept-Language':'en-US,en;q=0.9',\n",
    "        #     'Cache-Control': 'max-age=0',\n",
    "        #     'Cookie': 'Here is where I copied the cookies from my browser, I looked through it and it contained some info that Might be able to personally identify me so I removed it from the post',\n",
    "        #     'Sec-Ch-Ua': '\"Not.A/Brand\";v=\"8\", \"Chromium\";v=\"114\", \"Google Chrome\";v=\"114\"',\n",
    "        #     'Sec-Ch-Ua-Mobile':'?0',\n",
    "        #     'Sec-Ch-Ua-Platform':\"Windows\",\n",
    "        #     'Sec-Fetch-Dest': 'document',\n",
    "        #     'Sec-Fetch-Mode': 'navigate',\n",
    "        #     'Sec-Fetch-Site': 'same-origin',\n",
    "        #     'Sec-Fetch-User':'?1',\n",
    "        #     'Upgrade-Insecure-Requests': '1',\n",
    "        #     'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'\n",
    "        # }\n",
    "        \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'\n",
    "        }\n",
    "\n",
    "        # Send a GET request to the URL with headers\n",
    "        response = requests.get(\n",
    "            url, \n",
    "            headers=headers\n",
    "        )\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code != 200:\n",
    "            return response.status_code\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Function to extract and clean text from a list of elements\n",
    "        def extract_text(elements):\n",
    "            return [element.get_text(separator=\" \").strip() for element in elements if element.get_text().strip()]\n",
    "\n",
    "        # Extract headings, subheadings, paragraphs, and list items\n",
    "        headings = extract_text(soup.find_all('h1'))\n",
    "        paragraphs = extract_text(soup.find_all('p'))\n",
    "        list_items = extract_text(soup.find_all('li'))\n",
    "        articles = extract_text(soup.find_all('article'))\n",
    "        sections = extract_text(soup.find_all('section'))\n",
    "        block_quote = extract_text(soup.find_all('blockquote'))\n",
    "        tables = extract_text(soup.find_all('tr'))\n",
    "\n",
    "\n",
    "\n",
    "        # Combine all extracted text\n",
    "        all_text = headings+paragraphs+articles+block_quote+sections+tables+list_items\n",
    "        \n",
    "        # Join the lines back together without empty lines\n",
    "        result_string = ' '.join(all_text)\n",
    "\n",
    "        # Clean up any extra whitespace\n",
    "        formatted_text = re.sub(\n",
    "            r'\\s+', ' ', \n",
    "            result_string\n",
    "        )\n",
    "\n",
    "        print(formatted_text)\n",
    "        \n",
    "        self.__write_txt_file(\n",
    "            text=formatted_text,\n",
    "            string = result_string\n",
    "        )\n",
    "        \n",
    "        return response.status_code\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrp = Scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrp.scrape_website(url='https://www.fiverr.com/users/junaidullhassan/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res= <Response [403]>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://hayabusa.open2ch.net/test/read.cgi/livejupiter/1700322786/l50'\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Referer': 'https://www.google.com/'\n",
    "}\n",
    "\n",
    "with requests.Session() as session:\n",
    "    session.headers.update(headers)\n",
    "    res = session.get(url)\n",
    "print('res=', res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import AsyncHtmlLoader,MHTMLLoader,UnstructuredHTMLLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredHTMLLoader('/home/junaid-ul-hassan/Documents/gpt.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = data.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = dict['page_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You said:\n",
      "\n",
      "I want fetch text current open tab in chrome extension using javascript how I can?\n",
      "\n",
      "ChatGPT said:\n",
      "\n",
      "To fetch the text of the currently open tab in a Chrome extension, you'll need to follow a few steps using the Chrome Extensions API, particularly the chrome.tabs API. Here’s a basic outline of how you can achieve this:\n",
      "\n",
      "Steps to Fetch Text from the Current Tab in a Chrome Extension:\n",
      "\n",
      "Add Necessary Permissions in manifest.json: You'll need the appropriate permissions in your manifest.json file to interact with the current tab.\n",
      "\n",
      "json\n",
      "\n",
      "{ \"manifest_version\": 3, \"name\": \"Fetch Tab Text\", \"version\": \"1.0\", \"permissions\": [ \"activeTab\", \"scripting\" ], \"background\": { \"service_worker\": \"background.js\" }, \"action\": { \"default_popup\": \"popup.html\" } }\n",
      "\n",
      "Create a Background Script (background.js): You’ll use the chrome.scripting.executeScript() method to inject JavaScript into the current tab. This will allow you to fetch the text from the DOM.\n",
      "\n",
      "javascript\n",
      "\n",
      "chrome.action.onClicked.addListener((tab) => { chrome.scripting.executeScript({ target: { tabId: tab.id }, function: getTextFromPage }); }); function getTextFromPage() { let bodyText = document.body.innerText; console.log(bodyText); alert(bodyText); // Or send it back to your extension return bodyText; }\n",
      "\n",
      "Popup Script (Optional): If you want to display the fetched text in a popup or perform further actions, you can create a simple popup.html and popup.js file.\n",
      "\n",
      "popup.html:\n",
      "\n",
      "html\n",
      "\n",
      "<html> <body> <button id=\"fetch-text\">Fetch Text</button> <pre id=\"output\"></pre> <script src=\"popup.js\"></script> </body> </html>\n",
      "\n",
      "popup.js:\n",
      "\n",
      "javascript\n",
      "\n",
      "document.getElementById('fetch-text').addEventListener('click', () => { chrome.tabs.query({ active: true, currentWindow: true }, (tabs) => { chrome.scripting.executeScript({ target: { tabId: tabs[0].id }, function: getTextFromPage }, (results) => { document.getElementById('output').textContent = results[0].result; }); }); }); function getTextFromPage() { return document.body.innerText; }\n",
      "\n",
      "Explanation:\n",
      "\n",
      "Permissions: You need the activeTab permission to interact with the active tab.\n",
      "\n",
      "chrome.scripting.executeScript: This allows you to inject JavaScript into the active tab, where you can access the DOM and fetch the text.\n",
      "\n",
      "getTextFromPage: This function fetches the text from the document.body of the current tab.\n",
      "\n",
      "This setup will enable your Chrome extension to fetch the visible text from the active tab and display or manipulate it.\n",
      "\n",
      "You said:\n",
      "\n",
      "text should be fetch automatically when user open chrome extension\n",
      "\n",
      "ChatGPT said:\n",
      "\n",
      "To fetch the text automatically when a user opens the Chrome extension, you need to execute the script when the extension popup is opened, without requiring a button click. You can achieve this by immediately running the script inside the popup’s JavaScript file.\n",
      "\n",
      "Here’s how you can modify the extension to automatically fetch the text as soon as the user opens the extension:\n",
      "\n",
      "1. manifest.json File\n",
      "\n",
      "Make sure you have the necessary permissions and popup configured correctly:\n",
      "\n",
      "json\n",
      "\n",
      "{ \"manifest_version\": 3, \"name\": \"Fetch Tab Text\", \"version\": \"1.0\", \"permissions\": [ \"activeTab\", \"scripting\" ], \"background\": { \"service_worker\": \"background.js\" }, \"action\": { \"default_popup\": \"popup.html\" } }\n",
      "\n",
      "2. Remove Button and Add Auto Execution in Popup Script\n",
      "\n",
      "In the popup HTML and JavaScript, remove the button and execute the script when the popup opens.\n",
      "\n",
      "popup.html:\n",
      "\n",
      "This is a simple HTML file with an area to display the fetched text. No button is needed since the text will be fetched automatically.\n",
      "\n",
      "html\n",
      "\n",
      "<!DOCTYPE html> <html> <head> <title>Fetch Tab Text</title> </head> <body> <pre id=\"output\">Fetching text...</pre> <script src=\"popup.js\"></script> </body> </html>\n",
      "\n",
      "popup.js:\n",
      "\n",
      "In the popup script, as soon as the popup loads, it will fetch the text from the active tab and display it.\n",
      "\n",
      "javascript\n",
      "\n",
      "// Automatically fetch text when the popup is opened window.onload = function() { chrome.tabs.query({ active: true, currentWindow: true }, (tabs) => { chrome.scripting.executeScript({ target: { tabId: tabs[0].id }, function: getTextFromPage }, (results) => { // Display the fetched text in the popup document.getElementById('output').textContent = results[0].result; }); }); }; // Function to fetch the text from the current tab function getTextFromPage() { return document.body.innerText; }\n",
      "\n",
      "3. How It Works:\n",
      "\n",
      "Popup Load Trigger: As soon as the user opens the Chrome extension (i.e., when the popup is opened), the window.onload function is triggered.\n",
      "\n",
      "chrome.tabs.query: This queries for the active tab in the current window.\n",
      "\n",
      "chrome.scripting.executeScript: This injects the getTextFromPage function into the active tab, which fetches the text content of the body (document.body.innerText).\n",
      "\n",
      "Display the Text: The fetched text is then automatically displayed in the pre element of the popup.\n",
      "\n",
      "This setup eliminates the need for a button, and the extension will fetch and display the text immediately when the user opens the extension.\n",
      "\n",
      "You said:\n",
      "\n",
      "popup.js\n",
      "\n",
      "document.addEventListener('DOMContentLoaded', processURL);\n",
      "document.getElementById('send-btn').addEventListener('click', sendMessage);\n",
      "document.getElementById('user-input').addEventListener('keypress', checkEnter);\n",
      "document.getElementById('clear-chat-btn').addEventListener('click', clearChatHistory);\n",
      "document.getElementById('dark-theme').addEventListener('click', toggleDarkTheme);\n",
      "\n",
      "async function processURL() {\n",
      "    // Get the current tab's URL\n",
      "    const [tab] = await chrome.tabs.query({ active: true, currentWindow: true });\n",
      "    const url = tab.url;\n",
      "    document.getElementById('web-link').textContent = url;\n",
      "\n",
      "    // Check for irrelevant URLs\n",
      "    if (isIrrelevantTab(url)) {\n",
      "        showError(\"This is an irrelevant tab. Please open a valid website.\");\n",
      "        return; // Exit if the tab is irrelevant\n",
      "    }\n",
      "\n",
      "    // Re-enable input if valid tab\n",
      "    disableChatInput(false);\n",
      "\n",
      "    // Send the URL to the Flask server to process it\n",
      "    await fetch('http://127.0.0.1:5000/process_url', {\n",
      "        method: 'POST',\n",
      "        headers: {\n",
      "            'Content-Type': 'application/json'\n",
      "        },\n",
      "        body: JSON.stringify({ url: url })\n",
      "    });\n",
      "}\n",
      "\n",
      "background.js\n",
      "\n",
      "chrome.runtime.onInstalled.addListener(() => {\n",
      "    console.log('Web Pilot extension installed');\n",
      "});\n",
      "\n",
      "chrome.action.onClicked.addListener((tab) => {\n",
      "    chrome.scripting.executeScript({\n",
      "        target: { tabId: tab.id },\n",
      "        files: ['popup.js']\n",
      "    });\n",
      "});\n",
      "\n",
      "chrome.tabs.onUpdated.addListener((tabId, changeInfo, tab) => {\n",
      "    if (changeInfo.status === 'complete' && tab.active) {\n",
      "        chrome.tabs.query({ active: true, currentWindow: true }, (tabs) => {\n",
      "            let activeTab = tabs[0];\n",
      "            let activeTabId = activeTab.id;\n",
      "            chrome.tabs.sendMessage(activeTabId, { message: \"Tab updated\", url: activeTab.url });\n",
      "        });\n",
      "    }\n",
      "});\n",
      "\n",
      "modify this code I also want to fetch current open tab all text along with URL and then send this data to flask server.\n",
      "\n",
      "ChatGPT said:\n",
      "\n",
      "To modify the code so that it fetches both the current tab’s URL and all the text from the page and then sends that data to the Flask server, you'll need to:\n",
      "\n",
      "Inject a script to retrieve the text from the current tab.\n",
      "\n",
      "Modify the processURL function to gather both the URL and the page's text.\n",
      "\n",
      "Send the combined data (URL + page text) to your Flask server.\n",
      "\n",
      "Updated popup.js:\n",
      "\n",
      "We'll make the following updates:\n",
      "\n",
      "Inject a script to fetch the page text.\n",
      "\n",
      "Combine the URL and text into a single payload to send to the Flask server.\n",
      "\n",
      "javascript\n",
      "\n",
      "document.addEventListener('DOMContentLoaded', processPage); document.getElementById('send-btn').addEventListener('click', sendMessage); document.getElementById('user-input').addEventListener('keypress', checkEnter); document.getElementById('clear-chat-btn').addEventListener('click', clearChatHistory); document.getElementById('dark-theme').addEventListener('click', toggleDarkTheme); async function processPage() { // Get the current tab's URL const [tab] = await chrome.tabs.query({ active: true, currentWindow: true }); const url = tab.url; document.getElementById('web-link').textContent = url; // Check for irrelevant URLs if (isIrrelevantTab(url)) { showError(\"This is an irrelevant tab. Please open a valid website.\"); return; // Exit if the tab is irrelevant } // Inject script to fetch the page's text content const [result] = await chrome.scripting.executeScript({ target: { tabId: tab.id }, func: fetchPageText }); const pageText = result.result; // Combine URL and page text into a single payload const payload = { url: url, text: pageText }; // Send the data (URL + page text) to the Flask server await fetch('http://127.0.0.1:5000/process_page', { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify(payload) }); } // Function to fetch all visible text from the page function fetchPageText() { return document.body.innerText; }\n",
      "\n",
      "Explanation:\n",
      "\n",
      "processPage function:\n",
      "\n",
      "It fetches the current tab's URL and displays it.\n",
      "\n",
      "It checks if the tab is valid using the isIrrelevantTab function.\n",
      "\n",
      "Then, it injects the fetchPageText function into the active tab to retrieve the full visible text of the page.\n",
      "\n",
      "The result (page text) is combined with the URL into a single payload, which is sent to your Flask server via a POST request.\n",
      "\n",
      "fetchPageText function:\n",
      "\n",
      "This function is injected into the active tab and retrieves all visible text using document.body.innerText.\n",
      "\n",
      "Data Sent to Flask:\n",
      "\n",
      "A combined payload containing both the URL and the page's text is sent to the Flask server at 'http://127.0.0.1:5000/process_page'.\n",
      "\n",
      "Updated background.js:\n",
      "\n",
      "The background.js file doesn't need to inject popup.js anymore, as the text-fetching logic is directly triggered in popup.js. You can simplify it like this:\n",
      "\n",
      "javascript\n",
      "\n",
      "chrome.runtime.onInstalled.addListener(() => { console.log('Web Pilot extension installed'); }); // Optional: Listen for tab updates and trigger actions if necessary chrome.tabs.onUpdated.addListener((tabId, changeInfo, tab) => { if (changeInfo.status === 'complete' && tab.active) { chrome.tabs.query({ active: true, currentWindow: true }, (tabs) => { let activeTab = tabs[0]; let activeTabId = activeTab.id; chrome.tabs.sendMessage(activeTabId, { message: \"Tab updated\", url: activeTab.url }); }); } });\n",
      "\n",
      "Flask Server (Optional Update):\n",
      "\n",
      "Ensure your Flask server is ready to receive the combined data:\n",
      "\n",
      "python\n",
      "\n",
      "from flask import Flask, request, jsonify app = Flask(__name__) @app.route('/process_page', methods=['POST']) def process_page(): data = request.get_json() url = data['url'] text = data['text'] # Process the URL and page text as needed print(f\"URL: {url}\") print(f\"Text: {text[:100]}...\") # Just print the first 100 characters for brevity return jsonify({\"status\": \"success\"}) if __name__ == '__main__': app.run(debug=True)\n",
      "\n",
      "Summary of Changes:\n",
      "\n",
      "You inject a script into the active tab to fetch both the URL and visible text.\n",
      "\n",
      "The combined data is sent to your Flask server for processing.\n",
      "\n",
      "Now, when you open the\n",
      "\n",
      "You said:\n",
      "\n",
      "from flask import Flask, request, jsonify\n",
      "from RAG_QnA import RAG_Model\n",
      "from scrap import Scraper\n",
      "import requests\n",
      "import re\n",
      "import json\n",
      "\n",
      "app = Flask(__name__)\n",
      "\n",
      "def is_valid_url(url):\n",
      "    regex = re.compile(\n",
      "        r'^(?:http|ftp)s?://'  # http:// or https://\n",
      "        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|'  # domain...\n",
      "        r'localhost|'  # localhost...\n",
      "        r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}|'  # ...or ipv4\n",
      "        r'\\[?[A-F0-9]*:[A-F0-9:]+\\]?)'  # ...or ipv6\n",
      "        r'(?::\\d+)?'  # optional port\n",
      "        r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\n",
      "    return re.match(regex, url) is not None\n",
      "\n",
      "def is_pdf_url(url):\n",
      "    if url.lower().endswith('.pdf'):\n",
      "        return True\n",
      "    try:\n",
      "        response = requests.head(\n",
      "            url=url, \n",
      "            allow_redirects=True\n",
      "        )\n",
      "        content_type = response.headers.get('Content-Type', '')\n",
      "        is_doc = content_type.lower() == 'application/pdf'\n",
      "        print(\"Is Doc: \",is_doc)\n",
      "        return is_doc\n",
      "\n",
      "    except requests.RequestException as e:\n",
      "        print(f\"Request failed: {e}\")\n",
      "        return False\n",
      "\n",
      "def is_youtube_url(url):\n",
      "    youtube_regex = re.compile(\n",
      "        r'(https?://)?(www\\.)?(youtube|youtu|youtube-nocookie)\\.(com|be)/.+',\n",
      "        re.IGNORECASE\n",
      "    )\n",
      "    return re.match(youtube_regex, url) is not None\n",
      "\n",
      "\n",
      "\n",
      "# Initialize models\n",
      "scrp = Scraper()\n",
      "rag = RAG_Model()\n",
      "\n",
      "\n",
      "@app.route('/process_url', methods=['POST'])\n",
      "def process_url():\n",
      "    try:\n",
      "        flag1=False\n",
      "        flag2 = False\n",
      "        \n",
      "        data = request.json\n",
      "        text = data.get('text')\n",
      "        url = data.get('url')\n",
      "        \n",
      "        print(url)\n",
      "        print(text)\n",
      "        \n",
      "        if not url:\n",
      "            return jsonify({\n",
      "                'error': 'Missing URL'\n",
      "            }), 400\n",
      "        \n",
      "        if is_pdf_url(url=url):\n",
      "            flag1=True\n",
      "        elif is_youtube_url(url=url):\n",
      "            flag2=True\n",
      "            \n",
      "        if flag1:\n",
      "            rag.load_Database(\n",
      "                is_pdf=True,\n",
      "                pdf_url=url\n",
      "            )\n",
      "        elif flag2:\n",
      "            rag.load_Database(\n",
      "                is_youtube_url=True,\n",
      "                youtube_url=url\n",
      "            )\n",
      "        else:\n",
      "            scrp.Tab_data(text=text)\n",
      "            rag.load_Database()\n",
      "\n",
      "        return jsonify({\n",
      "            'message': 'URL processed successfully'\n",
      "        }), 200\n",
      "\n",
      "    except Exception as e:\n",
      "        return jsonify({\n",
      "            'error': str(e)\n",
      "        }), 500\n",
      "\n",
      "@app.route('/generate_response', methods=['POST'])\n",
      "def generate_response():\n",
      "    try:\n",
      "        data = request.json\n",
      "        user_input = data.get('message')\n",
      "\n",
      "        if not user_input:\n",
      "            return jsonify({\n",
      "                'error': 'Missing message'\n",
      "            }), 400\n",
      "\n",
      "        # Generate response based on user input\n",
      "        response = rag.generateResponse(user_input)\n",
      "        \n",
      "        return jsonify({'response': response}), 200\n",
      "\n",
      "    except Exception as e:\n",
      "        return jsonify({'error': str(e)}), 500\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    app.run(debug=False)\n",
      "\n",
      "this is my flask code plz modify it according to above extension code. It should receive URL and text from popup.js code\n",
      "\n",
      "ChatGPT said:\n",
      "\n",
      "Here's the modified Flask code to handle the URL and text from your Chrome extension's popup.js. The main change is to ensure the /process_url endpoint accepts both URL and text from the extension and processes them accordingly.\n",
      "\n",
      "Modified Flask Code:\n",
      "\n",
      "python\n",
      "\n",
      "from flask import Flask, request, jsonify from RAG_QnA import RAG_Model from scrap import Scraper import requests import re import json app = Flask(__name__) # Utility functions to validate URLs def is_valid_url(url): regex = re.compile( r'^(?:http|ftp)s?://' # http:// or https:// r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|' # domain... r'localhost|' # localhost... r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}|' # ...or ipv4 r'\\[?[A-F0-9]*:[A-F0-9:]+\\]?)' # ...or ipv6 r'(?::\\d+)?' # optional port r'(?:/?|[/?]\\S+)$', re.IGNORECASE) return re.match(regex, url) is not None def is_pdf_url(url): if url.lower().endswith('.pdf'): return True try: response = requests.head( url=url, allow_redirects=True ) content_type = response.headers.get('Content-Type', '') is_doc = content_type.lower() == 'application/pdf' print(\"Is Doc: \", is_doc) return is_doc except requests.RequestException as e: print(f\"Request failed: {e}\") return False def is_youtube_url(url): youtube_regex = re.compile( r'(https?://)?(www\\.)?(youtube|youtu|youtube-nocookie)\\.(com|be)/.+', re.IGNORECASE ) return re.match(youtube_regex, url) is not None # Initialize models scrp = Scraper() rag = RAG_Model() @app.route('/process_page', methods=['POST']) def process_page(): try: # Fetch URL and text from the request data = request.json url = data.get('url') text = data.get('text') if not url: return jsonify({'error': 'Missing URL'}), 400 if not text: return jsonify({'error': 'Missing text'}), 400 print(f\"URL: {url}\") print(f\"Text snippet: {text[:100]}\") # Print first 100 characters of the text for debugging # Check if the URL is a PDF or a YouTube link if is_pdf_url(url): # Process PDF rag.load_Database(is_pdf=True, pdf_url=url) elif is_youtube_url(url): # Process YouTube video rag.load_Database(is_youtube_url=True, youtube_url=url) else: # Process standard web page text scrp.Tab_data(text=text) rag.load_Database() return jsonify({'message': 'Page processed successfully'}), 200 except Exception as e: return jsonify({'error': str(e)}), 500 @app.route('/generate_response', methods=['POST']) def generate_response(): try: data = request.json user_input = data.get('message') if not user_input: return jsonify({'error': 'Missing message'}), 400 # Generate response based on user input response = rag.generateResponse(user_input) return jsonify({'response': response}), 200 except Exception as e: return jsonify({'error': str(e)}), 500 if __name__ == '__main__': app.run(debug=False)\n",
      "\n",
      "Key Changes:\n",
      "\n",
      "Endpoint Change:\n",
      "\n",
      "/process_page replaces /process_url to better reflect that it's processing both URL and page text.\n",
      "\n",
      "It now expects both url and text in the POST request from the Chrome extension.\n",
      "\n",
      "Data Handling:\n",
      "\n",
      "The process_page function now processes both the URL and the page text. It checks if the URL is a PDF or YouTube link, and if not, it processes the plain text scraped from the tab using scrp.Tab_data(text).\n",
      "\n",
      "Error Handling:\n",
      "\n",
      "If the url or text is missing from the POST request, it returns an error response.\n",
      "\n",
      "Debugging:\n",
      "\n",
      "The first 100 characters of the page text are printed for debugging purposes to confirm the received text.\n",
      "\n",
      "Extension Code Compatibility:\n",
      "\n",
      "The extension code you provided will send both the URL and the page's visible text to this /process_page endpoint as expected. The Flask server will then determine if the URL is a PDF, YouTube link, or a standard web page and process it accordingly.\n",
      "\n",
      "Expected POST Request from Chrome Extension:\n",
      "\n",
      "The popup.js will send a POST request to the Flask server like this:\n",
      "\n",
      "json\n",
      "\n",
      "{ \"url\": \"https://example.com\", \"text\": \"This is the visible text from the web page...\" }\n",
      "\n",
      "This data will be received and processed by the /process_page endpoint.\n",
      "\n",
      "ChatGPT can make mistakes. Check important info.\n"
     ]
    }
   ],
   "source": [
    "print(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text data saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Saving it to a text file\n",
    "with open('output.txt', 'w') as file:\n",
    "    file.write(text_data)\n",
    "\n",
    "print(\"Text data saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5041389,
     "sourceId": 8458105,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 1902,
     "modelInstanceId": 3900,
     "sourceId": 5112,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
